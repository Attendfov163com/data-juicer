<View>
  <Header value="Human Preference Annotation"/>
  <Text name="prompt" value="$prompt"/>

  <Header value="Compare these responses:"/>
  <Pairwise name="pairwise" toName="answer1,answer2">
    <Text name="answer1" value="$answer1"/>
    <Text name="answer2" value="$answer2"/>
  </Pairwise>

  <View style="box-shadow: 2px 2px 5px #999; padding: 20px; margin-top: 20px; border-radius: 5px;">
    <Header value="Which response is better?"/>
    <Choices name="preference" toName="pairwise" choice="single" showInLine="true">
      <Choice value="answer1" hotkey="1">Response 1</Choice>
      <Choice value="answer2" hotkey="2">Response 2</Choice>
    </Choices>
  </View>
</View>
<!--{ "data" : {
  "prompt": "What are the key benefits of using Reinforcement Learning from Human Feedback (RLHF) for dataset collection in the context of Large Language Model (LLM) generation?",
  "answer1": "Reinforcement Learning from Human Feedback (RLHF) for dataset collection in Large Language Model (LLM) generation provides key benefits such as improved model performance through direct optimization, better alignment with human values by incorporating human feedback, and the ability to iteratively refine the model based on user interactions, resulting in a more user-friendly and efficient language model.",
  "answer2": "Using Reinforcement Learning from Human Feedback (RLHF) for dataset collection in Large Language Model (LLM) generation offers advantages such as enhanced model capabilities by optimizing for desired outcomes, greater adaptability to human preferences through the inclusion of human feedback, and the opportunity to continuously improve the model based on user experiences, ultimately leading to a more effective and responsive language model."
}}
-->
