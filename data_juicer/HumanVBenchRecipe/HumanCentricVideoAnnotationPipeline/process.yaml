# Process config example including:
#   - all global arguments
#   - all ops and their arguments

# global parameters
project_name: 'all'                                         # project name for distinguish your configs
dataset_path: 'input.jsonl' 

export_path: 'output.jsonl'
np: 1                                                   # number of subprocess to process your dataset
                                                            # Note: currently, we support specify only ONE key for each op, for cases requiring multiple keys, users can specify the op multiple times. We will only use the first key of `text_keys` when you set multiple keys.
open_tracer: false                                          # whether to open the tracer to trace the changes during process. It might take more time when opening tracer

# for multimodal data processing
video_key: 'videos'                                         # key name of field to store the list of sample video paths.
video_special_token: '<__dj__video>'                        # the special token that represents a video in the text. In default, it's "<__dj__video>". You can specify your own special token according to your input dataset.

eoc_special_token: '<|__dj__eoc|>'                          # the special token that represents the end of a chunk in the text. In default, it's "<|__dj__eoc|>". You can specify your own special token according to your input dataset.

# process schedule: a list of several process operators with their arguments
# hyperparameters are set according to the 3-sigma stats on MSR-VTT dataset
process:
  # - video_resolution_filter:                                # filter samples according to the resolution of videos in them
  #     min_width: 1280                                         # the min resolution of horizontal resolution filter range (unit p)
  #     max_width: 4096                                         # the max resolution of horizontal resolution filter range (unit p)
  #     min_height: 480                                         # the min resolution of vertical resolution filter range (unit p)
  #     max_height: 1080                                        # the max resolution of vertical resolution filter range (unit p)
  #     any_or_all: any   

  # - video_aesthetics_filter:                                # filter samples according to the aesthetics score of frame images extracted from videos.
  #     hf_scorer_model: shunk031/aesthetics-predictor-v2-sac-logos-ava1-l14-linearMSE # Huggingface model name for the aesthetics predictor
  #     min_score: 0.3                                          # the min aesthetics score of filter range
  #     max_score: 1.0                                          # the max aesthetics score of filter range
  #     frame_sampling_method: 'uniform'                        # sampling method of extracting frame images from the videos. Should be one of ["all_keyframe", "uniform"]. The former one extracts all key frames and the latter one extract specified number of frames uniformly from the video. Default: "uniform" with frame_num=3, considering that the number of keyframes can be large while their difference is usually small in terms of their aesthetics.
  #     frame_num: 3                                            # the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is "uniform". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.
  #     reduce_mode: avg                                        # reduce mode to the all frames extracted from videos, must be one of ['avg','max', 'min'].
  #     any_or_all: any                                         # keep this sample when any/all images meet the filter condition
  #     mem_required: '1500MB' 

  # - video_nsfw_filter:                                      # filter samples according to the nsfw scores of videos in them
  #     hf_nsfw_model: Falconsai/nsfw_image_detection           # Huggingface model name for nsfw classification
  #     score_threshold: 0.5                                    # the nsfw score threshold for samples, range from 0 to 1. Samples with nsfw score less than this threshold will be kept.
  #     frame_sampling_method: all_keyframes                    # sampling method of extracting frame images from the videos. Should be one of ["all_keyframes", "uniform"]. The former one extracts all key frames and the latter one extract specified number of frames uniformly from the video. Default: "all_keyframes".
  #     frame_num: 3                                            # the number of frames to be extracted uniformly from the video. Only works when frame_sampling_method is "uniform". If it's 1, only the middle frame will be extracted. If it's 2, only the first and the last frames will be extracted. If it's larger than 2, in addition to the first and the last frames, other frames will be extracted uniformly within the video duration.
  #     reduce_mode: avg                                        # reduce mode for multiple sampled video frames to compute nsfw scores of videos, must be one of ['avg','max', 'min'].
  #     any_or_all: any                                         # keep this sample when any/all images meet the filter condition
  #     mem_required: '1GB'   

  # - video_split_by_scene_mapper:                            # split videos into scene clips
  #     detector: 'ContentDetector'                             # PySceneDetect scene detector. Should be one of ['ContentDetector', 'ThresholdDetector', 'AdaptiveDetector`]
  #     threshold: 27.0                                         # threshold passed to the detector
  #     min_scene_len: 15                                      # minimum length of any scene
  #     show_progress: false                                    # whether to show progress from scenedetect

#  reconstruct jsonl
#  data_juicer/HumanVBenchRecipe/HumanCentricVideoAnnotationPipeline/inter_function/after_split_reconstruct_jsonl.py

  # - video_duration_filter:                                  # Keep data samples whose videos' durations are within a specified range.
  #     min_duration: 2                                         # the min video duration of filter range (in seconds)
  #     max_duration: 100                                        # the max video duration of filter range (in seconds)
  #     any_or_all: any 

  # - video_motion_score_filter:
  #     min_score: 1.2
  #     max_score: 10000.0
  #     sampling_fps: 2
  #     any_or_all: any
  
  # - video_face_ratio_filter:  # 60
  #     threshold: 0.65
  #     detect_interval: 4
  #     any_or_all: any 

  # - video_human_tracks_extraction_mapper:
  #     face_track_bbox_path: your_path_to_save_bounding_box_data
  #     YOLOv8_human_model_path: ./data_juicer/my_pretrained_method/YOLOv8_human/weights/best.pt
  #     mem_required: '10GB'

  # - video_tagging_from_audio_mapper:                        # Mapper to generate video tags from audio streams extracted from the video.
  #     hf_ast: 'pt_model/ast-finetuned-audioset-10-10-0.4593'       # Huggingface model name for the audio classification model.
  #     mem_required: '500MB'  

  # - video_active_speaker_mapper:
  #     tempt_save_path: ./HumanVBenchRecipe/dj_ASD_tempt          # Used to store temporary videos
  #     face_track_bbox_path: ./HumanVBenchRecipe/dj_human_track        # Data storage address in video_human_tracks_extraction_mapper
  #     mem_required: '10GB' 

  # - video_audio_attribute_mapper:
  #     hf_audio_mapper: 'pt_model/wav2vec2-large-robust-24-ft-age-gender'      # Huggingface model name for speech age and gender classification
  #     mem_required: '7GB' 

  # - video_captioning_from_human_tracks_mapper:
  #     video_describe_model_path: pt_model/sharegpt4video-8b           # model path to sharegpt4video-8b
  #     tempt_video_path: data_juicer/HumanVBenchRecipe/dj_tmpt           # Used to store temporary videos
  #     mem_required: '35GB' 

  # - video_captioning_face_attribute_emotion_mapper:
  #     face_track_query: Please only describe the appearance and facial emotions of the person in the video in detail. Don't mention the background. Less than 80 words.
  #     cropping_face_video_tempt_path:./tempt_video/tmp_video_remove
  #     video_describe_model_path: 'pt_model/VideoLLaMA2'   # Huggingface model DAMO-NLP-SG/VideoLLaMA2-7B-16F
  #     mem_required: '35GB' 


  # - video_audio_speech_ASR_mapper:
  #     model_dir_ASR: 'pt_model/SenseVoiceSmall'
  #     mem_required: '20GB' 

  # - video_audio_speech_emotion_mapper:
  #     model_dir_emo: 'pt_model/SenseVoiceSmall'
  #     mem_required: '20GB' 

  # - video_captioning_mapper_T:
  #     query: 'Please describe the video, including its event, environment and atmosphere. Less than 200 words.'
  #     video_describe_model_path: 'pt_model/sharegpt4video-8b'         # model path to sharegpt4video-8b
  #     mem_required: '20GB' 