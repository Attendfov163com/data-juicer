# Process config example including:
#   - all global arguments
#   - all ops and their arguments

# global parameters
project_name: 'all'                                         # project name for distinguish your configs
dataset_path: '/home/daoyuan_mm/data_juicer/HumanVBenchRecipe/dj_process_flow/raw_videos_prcocess1_j.jsonl'  # '/mnt/zt_dj_output/pexels50k/pexels50k_mapper9.jsonl'
                                                            # accepted format: 'weight1(optional) dataset1-path weight2(optional) dataset2-path'
export_path: '/home/daoyuan_mm/data_juicer/HumanVBenchRecipe/dj_process_flow/raw_videos_prcocess2.jsonl'  # '/mnt/zt_dj_output/pexels50k/pexels50k_mapper10_activespeakernew.jsonl'  # /mnt/zt_dj_output/face_bbox_pexel230k/pexel230k_mapper5678.jsonl
np: 1                                                   # number of subprocess to process your dataset
                                                            # Note: currently, we support specify only ONE key for each op, for cases requiring multiple keys, users can specify the op multiple times. We will only use the first key of `text_keys` when you set multiple keys.
open_tracer: false                                          # whether to open the tracer to trace the changes during process. It might take more time when opening tracer

# for multimodal data processing
video_key: 'videos'                                         # key name of field to store the list of sample video paths.
video_special_token: '<__dj__video>'                        # the special token that represents a video in the text. In default, it's "<__dj__video>". You can specify your own special token according to your input dataset.

eoc_special_token: '<|__dj__eoc|>'                          # the special token that represents the end of a chunk in the text. In default, it's "<|__dj__eoc|>". You can specify your own special token according to your input dataset.

# process schedule: a list of several process operators with their arguments
# hyperparameters are set according to the 3-sigma stats on MSR-VTT dataset
process:
  # - video_split_by_scene_mapper:                            # split videos into scene clips
  #     detector: 'ContentDetector'                             # PySceneDetect scene detector. Should be one of ['ContentDetector', 'ThresholdDetector', 'AdaptiveDetector`]
  #     threshold: 27.0                                         # threshold passed to the detector
  #     min_scene_len: 15                                      # minimum length of any scene
  #     show_progress: false                                    # whether to show progress from scenedetect

  - video_duration_filter:                                  # Keep data samples whose videos' durations are within a specified range.
      min_duration: 2                                         # the min video duration of filter range (in seconds)
      max_duration: 100                                        # the max video duration of filter range (in seconds)
      any_or_all: any 

  # - video_motion_score_filter:
  #     min_score: 1.2
  #     max_score: 10000.0
  #     sampling_fps: 2
  #     any_or_all: any
  
  # - video_face_ratio_filter:  # 60
  #     threshold: 0.65
  #     detect_interval: 4
  #     any_or_all: any 

  # - video_tagging_from_audio_mapper:                        # Mapper to generate video tags from audio streams extracted from the video.
  #     hf_ast: '/mnt1/daoyuan_mm/ast-finetuned-audioset-10-10-0.4593'       # Huggingface model name for the audio classification model.
  #     mem_required: '500MB'  

  # - video_active_speaker_mapper: # 56
  #     tempt_save_path: '/mnt3/daoyuan_mm/dj_ASD_tempt'
  #     face_track_bbox_path: '/mnt/zt_dj_output/face_bbox_pexel230k'
  #     mem_required: '10GB' 

  # - tag_equal_filter:
  #     check_field: '__dj__ASD_attribute_'
  #     prohibit_content: [[]]
  # - video_audio_attribute_mapper:
  #     hf_audio_mapper: '/mnt1/daoyuan_mm/wav2vec2-large-robust-24-ft-age-gender'
  #     mem_required: '7GB' 

  # - video_captioning_mapper_T:
  #     query: 'Please describe the video, including its event, environment and atmosphere. Less than 200 words.'
  #     video_describe_model_path: '/mnt1/daoyuan_mm/sharegpt4video-8b'
  #     mem_required: '20GB' 

  # - tag_equal_filter:
  #     check_field: '__dj__video_caption_'
  #     prohibit_content: [["False"]]  

#   - video_captioning_from_human_tracks_mapper:
#       video_describe_model_path: '/mnt1/daoyuan_mm/sharegpt4video-8b'
#       tempt_video_path: '/mnt3/daoyuan_mm/dj_face_emotion_tmpt'
#       mem_required: '35GB' 
#   - video_captioning_face_attribute_emotion_mapper:
#       face_track_query: Please only describe the appearance and facial emotions of the person in the video in detail. Don't mention the background. Less than 80 words.
#       cropping_face_video_tempt_path: '/mnt3/daoyuan_mm/dj_face_emotion_tmpt'
#       video_describe_model_path: '/mnt1/daoyuan_mm/sharegpt4video-8b'
#       mem_required: '35GB' 

#   - video_captioning_face_attribute_demographic_mapper:
#       original_data_save_path: '/mnt3/daoyuan_mm/dj_faceattribute'
#       detect_interval: 5
#   - video_audio_speech_annotation_mapper:
#       model_dir_ASR_emo: '/mnt1/daoyuan_mm/SenseVoiceSmall'
#       mem_required: '20GB' 

  # - ASD_revise_mapper:
  #     acitve_threshold : 12

  # - Attribute_summarize_mapper:
  #     llama3_path: '/mnt/zt_pt_model/Meta-Llama-3.1-8B-Instruct'
  #     mem_required: '9GB' 

  # - text_emotion_classification_mapper:
  #     mem_required: '30GB' 

